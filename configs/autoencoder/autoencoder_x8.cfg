#
# TRAINING
#

[logging]
save_dir="checkpoints/autoencoder_x8-plain"
allow_overwrite=false
plot_epochs=50
save_epochs=250
temp_save_epochs=10
seed=74

[training]
train_batch_size=96
gradient_accumulation_steps=1
gradient_clip_val=0.16
mixed_precision=fp16
epochs=6500
epoch_steps=256
seed=74
sigma_data=0.5
kl_weight=0.01
mse_weight=1
percep_weight=1
disc_weight=0.0
adam_betas=[0.9, 0.999]
disc_adam_betas=[0, 0.999]
disc_lr_mult=0.5
lambda_gp=5

[ema]
sigma_rels=[0.05, 0.1]
update_every=20
checkpoint_every_num_steps=12800
allow_different_devices=true

[wandb]
project="generative_land"
tags=["64x64->512x512", "autoencoder"]
mode=online

[model]
@model=autoencoder
image_size=512
in_channels=1
out_channels=1
model_channels=64
model_channel_mults=[1, 2, 4, 4]
layers_per_block=2
emb_channels=null
noise_emb_dims=0
custom_cond_emb_dims=null
attn_resolutions=[]
midblock_attention=false
channels_per_head=64
concat_balance=0.5
#block_kwargs={"resample_type": "conv", "resample_filter": 2}
latent_channels=4

[discriminator]
@model=patchgan_discriminator
in_channels=1
ndf=64
n_layers=3

[lr_sched]
@lr_sched=sqrt
lr=0.003
# good value is ~70000 * batch_size * gradient_accumulation_steps
ref_nimg=3000000
#warmup_nimg=300000
warmup_nimg=300000

[train_dataset]
@dataset=multi_dataset
weights=[10, 1, 1]

# Land
[train_dataset.*.0]
@dataset=h5_autoencoder
h5_file="dataset_full.h5"
crop_size=128
pct_land_range=[0.9999, 1]
dataset_label="480m"

# Ocean
[train_dataset.*.1]
@dataset=h5_autoencoder
h5_file="dataset_full.h5"
crop_size=128
pct_land_range=[0, 0.9999]
dataset_label="480m"

# Land
[train_dataset.*.2]
@dataset=h5_autoencoder
h5_file="dataset_full.h5"
crop_size=128
pct_land_range=[0.9999, 1]
dataset_label="240m"

[dataloader_kwargs]
num_workers=4
persistent_workers=true
pin_memory=true
pin_memory_device=cuda
prefetch_factor=4