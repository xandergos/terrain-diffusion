#
# TRAINING
#

[logging]
save_dir="checkpoints/autoencoder_x8"
allow_overwrite=false
plot_epochs=50
save_epochs=5
temp_save_epochs=1
seed=74

[training]
train_batch_size=64
gradient_accumulation_steps=1
gradient_clip_val=5
mixed_precision=fp16
epochs=60
epoch_steps=1024
seed=74
sigma_data=0.5
kl_weight=0.1
direct_weight=1
percep_weight=1
warmup_steps=1024

[evaluation]
validate_epochs=5
val_ema_idx=0
validation_repeats=32
training_eval=true
mse_scale_eps=0.05

[optimizer]
type=adam
kwargs={"betas": [0.9, 0.99]}

[ema]
sigma_rels=[0.05, 0.1]
update_every=1
checkpoint_every_num_steps=1024

[wandb]
project="generative_land"
tags=["64x64->512x512", "autoencoder"]
mode=online
save_code=true

[model]
@model=autoencoder
image_size=512
in_channels=2
out_channels=2
model_channels=64
model_channel_mults=[1, 2, 4, 4]
layers_per_block=2
emb_channels=null
noise_emb_dims=0
custom_cond_emb_dims=null
attn_resolutions=[]
midblock_attention=false
concat_balance=0.5
latent_channels=4
conditional_inputs=[]

[lr_sched]
@lr_sched=sqrt
lr=0.01
ref_nimg=327680
warmup_nimg=128000

[train_dataset]
@dataset=h5_autoencoder
h5_file="dataset.h5"
crop_size=128
pct_land_ranges=[[0, 1], [0, 1], [0, 1]]
subset_resolutions=[90, 180, 360]
subset_weights=[4, 2, 1]
sigma_data=0.5
split="train"
use_watercover=true
require_watercover=false

[val_dataset]
@dataset=h5_autoencoder
h5_file="dataset.h5"
crop_size=128
pct_land_ranges=[[0, 1], [0, 1], [0, 1]]
subset_resolutions=[90, 180, 360]
subset_weights=[4, 2, 1]
sigma_data=0.5
split="val"
use_watercover=true
require_watercover=false

[dataloader_kwargs]
num_workers=4
pin_memory=true
prefetch_factor=4