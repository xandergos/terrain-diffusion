#
# TRAINING
#

[logging]
save_dir="checkpoints/64_128_phema"
allow_overwrite=false
plot_epochs=50
save_epochs=100
temp_save_epochs=10
seed=74

[training]
train_batch_size=96
gradient_accumulation_steps=2
mixed_precision=fp16
epochs=6500
epoch_steps=128
seed=74
# Mean of log noise during training
P_mean=-0.4  
# STD of log noise during training
P_std=1.4
# This is to allow placing more emphasis on the higher frequency channels
channel_max_weight=[10000, 100]
# probability a sigma is replaced with a random uniform value, to ensure very large sigmas are reached
random_replace_rate=0.001

[ema]
sigma_rels=[0.05, 0.1]
update_every=20
checkpoint_every_num_steps=12800
allow_different_devices=true

[wandb]
project="generative_land"
tags=["64x64"]
mode=online

[model]
@model=unet
image_size=64
in_channels=3
out_channels=2
model_channels=128
model_channel_mults=[1, 2, 3, 4]
layers_per_block=3
emb_channels=null
noise_emb_dims=null
attn_resolutions=[8, 16]
midblock_attention=true
channels_per_head=64
concat_balance=0.5
# label (0 or 1) + conditional noise label
conditional_inputs=[["embedding", 2, 1], ["float", 64, 0.5]]

[lr_sched]
@lr_sched=sqrt
lr=0.005
# good value is ~70000 * batch_size * gradient_accumulation_steps
ref_nimg=10000000
warmup_nimg=1000000

[train_dataset]
@dataset=multi_dataset
weights=[10, 1, 10]
labels=[0, 0, 1]

# Land
[train_dataset.*.0]
@dataset=h5_base_terrain
h5_file="dataset64.h5"
crop_size=64
pct_land_range=[0.9999, 1]
dataset_label="64"
model_label=0

# Ocean
[train_dataset.*.1]
@dataset=h5_base_terrain
h5_file="dataset64.h5"
crop_size=64
pct_land_range=[0, 0.9999]
dataset_label="64"
model_label=0

# Land but higher res
[train_dataset.*.2]
@dataset=h5_base_terrain
h5_file="dataset64.h5"
crop_size=64
pct_land_range=[0.9999, 1]
dataset_label="256"
model_label=1

[dataloader_kwargs]
num_workers=4
persistent_workers=true
pin_memory=true
pin_memory_device=cuda
prefetch_factor=4


#
# TRAINING AND SAMPLING
#

[scheduler]
@scheduler=edm_dpm
sigma_min=0.002
sigma_max=80
sigma_data=0.5
scaling_p=2
scaling_t=0.01

[sampler]
region=[0, 0, 64, 64]

[sampler.init]
@sampler=tiled
boundary=[0, 0, 64, 64]
timesteps=20
batch_size=64
generation_batch_size=2
device=cuda
seed=613414
overlap=16
[sampler.init.model]
@utils=get_object
object=model
[sampler.init.scheduler]
@utils=get_object
object=scheduler

[sampler.init.postprocessor]
@postprocessor=decode
denoise=true

# Should match the training dataset
[sampler.init.postprocessor.encoder]
@encoder=laplacian_pyramid_encoder
resize_scales=[1]
sigma=[5]
raw_mean=[0, -2651]
raw_std=[160, 2420]

[sampler.init.network_inputs]
@network_inputs=base_terrain_nocond
label=0
image_size=64